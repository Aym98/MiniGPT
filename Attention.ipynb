{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to implement part of the famous paper \"Attention Is All You Need\". We will try to create a text completion generator trained on a part of the dialogue of the Naruto series (Naruto and Nagato/Pain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x201856dde30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1337\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "n_steps = 5000\n",
    "eval_iter = 200\n",
    "eval_inter = 500\n",
    "n_embeds = 384\n",
    "block_size = 56\n",
    "n_heads = 6\n",
    "n_layer = 6\n",
    "head_size = 16\n",
    "dropout = 0.2\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***First step: Getting the data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first read the text file and save it into a variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"NarutoPain.txt\"\n",
    "with open(input, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Second Step: Tokenization***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to chose tokens. Let's take the simplest way and assume each character is a token.\n",
    "Now, let's take a sorted list of the characters used and compute our vocabulary size.\n",
    "Then, let's code the encoding and decoding functions that assigns numerical value to each characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary sizer is 59\n"
     ]
    }
   ],
   "source": [
    "list_chars = sorted(list(set(text)))\n",
    "vocabulary_size = len(list_chars)\n",
    "print(f\"The vocabulary sizer is {vocabulary_size}\")\n",
    "\n",
    "enc_dic ={ch : i for i,ch in enumerate(list_chars)}\n",
    "deco_dic = {i : ch for i,ch in enumerate(list_chars)}\n",
    "encode = lambda s: [enc_dic[c] for c in s]\n",
    "decode = lambda int_l: \"\".join([deco_dic[i] for i in int_l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform our textual data (text var) to a more suitable format, i.e: a tensor. And then split it between training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "train_perc = 0.9 #Percentage of data dedicated to training\n",
    "train_size = int(train_perc * len(data))\n",
    "training_data = data[ : train_size]\n",
    "val_data = data[train_size : ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is ready, we need to code our model. The picture below shows the general architecture of the Tra,sformer presented in the paper. Let's inspect it block by block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model architecture](modelArchi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the picture, the Transformer is composed of two stacks, the Encoder (The stack in the left) and the Decoder (stack in the right). \n",
    "\n",
    "Encoder stack :  Maps the input sequence (x1, ..., xn) to a continuous representation (z1, ..., zn), where the input sequence is : inputs( from data) + postional encoding vector. The positional encoding vector is a vector that represents the relative position of the tokens. The positional encoding is mandatory to make use of the order of the sequence since there is no recurence in the encoder. \n",
    "\n",
    "Decoder stack : Maps the predicted sentence by the encoder (z1, ..., zn) to a new sequence (y1, ..., yn) predicting one sequence at a time, while having the output at time n-1 be part of the inputs for the prediction of the token yn. Example: to generate yn we have as an input (z1, ..., zm, y1, ..., yn-1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will only code the encoder stack. The encoder is composed of N blocks, each of them having Multi-head attention layer, and a feed forward layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is Attention ?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![attention layer](AttentionLayer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture above shows the architecture of the attention mechanism, following the equation : \n",
    "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_{k}}})V$,\n",
    "where Q, K are vectors of dimension $d_{k}$ and V a vector of dimension $d_{v}$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try and understand the intuition behind this mathematical equation. \n",
    "Suppose we have a sequence X of legth T, for example X = Pain (T = 5) that we model by $X_{encoded}= (845, 25, 13, 10, 0)$, to predict the next character(token), my model needs context on the entire sequence.\n",
    "The simplest way to get this context would be to average all the tokens, and we get : $X_{avg} = (178.6, 178.6, 178.6, 178.6)$.\n",
    "There are several problems with this, but the biggest one would be that the 4th token knows what the fifth token. If I want my model to predict the word \"pain\", he can't have the information that after the letter \"a\" commes an \"n\" as an input, so to remedy that, we only average each token with the past and we get \n",
    "$X_{avg} = (845, 435, 294.3, 220.75, 178.6)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code it bit by bit to better show its working :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_encoded is tensor([845.,  25.,  13.,  10.,   0.]) and X_avg is : tensor([845.0000, 435.0000, 294.3333, 223.2500, 178.6000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "seed = 1337\n",
    "torch.manual_seed(seed) # For repetability\n",
    "vector_size = 5\n",
    "x_encoded = [845, 25, 13, 10, 0]\n",
    "q = torch.tensor(x_encoded, dtype = torch.float)\n",
    "x_avg = torch.zeros(vector_size)\n",
    "for i in range(vector_size):\n",
    "    x_avg[i] = q[:i+1].mean()\n",
    "\n",
    "\n",
    "print(f\"X_encoded is {q} and X_avg is : {x_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this opperation more eficcient computationally, we use matrix multiplication.In order to do that, we multiply the sequence by a weight matrix such as: \n",
    "- All future tokens have a weight of 0.\n",
    "- All the weights add to 1.\n",
    "To do that we use to tricks:\n",
    "- A triangular matrix (All the values above the diagonal are null), example this 3 x 3 matrix : \n",
    "$\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "2 & 3 & 0\\\\\n",
    "4 & 5 & 6\n",
    "\\end{pmatrix}$\n",
    "and by replacing the zeros with $-\\infty$ since $softmax(-\\infty) = 0$\n",
    "- The softmax function to normalise the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_avg using softmax istensor([845.0000, 435.0000, 294.3334, 223.2500, 178.6000])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "weights = torch.tril(torch.ones(vector_size, vector_size))\n",
    "weights = weights.masked_fill(weights == 0, float(\"-inf\"))\n",
    "weights = F.softmax(weights, dim=1)\n",
    "x_avg_2 = weights @ q.transpose(-1, 0)\n",
    "print(f\"X_avg using softmax is{x_avg_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same but with a faster and more efficient methode. That's the secret behind the attention equation. The $\\sqrt{d_K}$ is to avoid having too much descrepency between weights of tokens and loosing information.\n",
    "Now, all we want is to have the weights for the operation be a function of the inputs, so we get $Q = f_1(inputs)$, $K = f_2(inputs)$, $weights = softmax(\\frac{Trig(QK^T)}{\\sqrt{d_K}})$ and $outputs = weights . f_3(inputs)$\n",
    "\n",
    "So, let's code our attention block :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "n_embeds = 384\n",
    "head_size = 16\n",
    "dropout = 0.2\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size) :\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embeds, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embeds, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embeds, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        #Compute initial weights or \"affinities\"\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the encoder we use multiheaded attention, which is simply the concatenation of multiple attention heads. So let's code it. (We also add a dropout layer to avoid overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 6\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.head = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embeds, n_embeds)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        y = torch.cat([h(x) for h in self.head], dim=-1)\n",
    "        y = self.proj(y)\n",
    "        y = self.dropout(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code the FeedForward layer and wrap this all up into a single block while also adding layer normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embeds):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embeds, 4*n_embeds),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embeds, n_embeds),\n",
    "            nn.Dropout(dropout),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads, n_embeds):\n",
    "        super().__init__()\n",
    "        head_size = n_embeds // n_heads\n",
    "        self.mheads = MultiHead(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embeds)\n",
    "        self.ln1 = nn.LayerNorm(n_embeds)\n",
    "        self.ln2 = nn.LayerNorm(n_embeds)\n",
    "    def forward(self, x):\n",
    "        x_sa = self.mheads(self.ln1(x)) + x\n",
    "        x_ffwd = self.ffwd(self.ln2(x_sa)) + x_sa\n",
    "        return x_ffwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's code our Transformer. We just need to add embedding layers (one for token embedding and the second for positional embedding) and a final linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 56\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, n_embeds)\n",
    "        self.positional_emb = nn.Embedding(block_size, n_embeds)\n",
    "        self.block = nn.Sequential(*[Block(n_heads, n_embeds) for _ in range(n_layer)])\n",
    "        self.lnorm = nn.LayerNorm(n_embeds)\n",
    "        self.linear = nn.Linear(n_embeds, vocabulary_size)\n",
    "       \n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        _, t = x.shape \n",
    "        x_t = torch.arange(t, device=device)\n",
    "        embedded_token = self.token_embedding_table(x) # shape B,T,C: n_embeds\n",
    "        embedded_pos = self.positional_emb(x_t)\n",
    "        x_pos = embedded_token + embedded_pos\n",
    "        x_pos = self.block(x_pos)\n",
    "        x_pos = self.lnorm(x_pos)\n",
    "        logits = self.linear(x_pos)  # shape B, T, vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:  # returns the loss\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss \n",
    "    \n",
    "    def generate(self, x, max_new_tokens):\n",
    "        x_cond = x[-block_size:]\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(x_cond)\n",
    "            logits = logits[:, -1, :] #Last elem from time dimension \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            x_next = torch.multinomial(probs, num_samples=1) # Chose the token from a multinomal distribution with probability law of probs\n",
    "            x = torch.cat((x, x_next), dim=1)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's code our training function to tran the model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch the data \n",
    "eval_iter = 200\n",
    "eval_inter = 50\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_batch(split):\n",
    "    data = training_data if split == \"train\" else val_data\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    x, y = x.to(device), y.to(device) # To run the code on GPU\n",
    "    return x,y\n",
    "\n",
    "#Loss estimation function \n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iter)\n",
    "        for k in range(eval_iter):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item() \n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def learning_rate_scheduler(num_step, lr):\n",
    "    if num_step < 50:\n",
    "        return lr\n",
    "    else : \n",
    "        return 3e-4\n",
    "    \n",
    "\n",
    "def train(model, num_steps):\n",
    "    history = {\"train_losses\" : [], \"val_losses\" : []}\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "    #evaluate loss on val too\n",
    "        if step % eval_inter == 0:\n",
    "            losses = estimate_loss(model)\n",
    "            history[\"train_losses\"].append(losses[\"train\"])\n",
    "            history[\"val_losses\"].append(losses[\"val\"])\n",
    "            print(f\" for step {step} training loss is {losses[\"train\"] : .4f} validation loss is {losses[\"val\"] : .4f}\")\n",
    "\n",
    "    #sample data \n",
    "        xb, yb = get_batch(\"train\")\n",
    "        optim = torch.optim.AdamW(model.parameters(), lr=learning_rate_scheduler(step, 1e-3))\n",
    "\n",
    "    #evaluate loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    return history "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialise and train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for step 0 training loss is  4.2643 validation loss is  4.2657\n",
      " for step 50 training loss is  2.3151 validation loss is  2.6014\n",
      " for step 100 training loss is  1.9899 validation loss is  2.3782\n",
      " for step 150 training loss is  1.8084 validation loss is  2.3568\n",
      " for step 200 training loss is  1.6092 validation loss is  2.3118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "naruto = TransformerModel().to(device)\n",
    "\n",
    "n_steps = 250\n",
    "\n",
    "history = train(naruto, num_steps=n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model \n",
    "torch.save(naruto.state_dict, \"NaruotVsPain.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namentn t d tsgtn  n   tdd  tttd xtt,ttttr .c ttttt!o! rsrrdt dnrt ottt  tt rtde,tnst! t tdtdtdt ptn    \n"
     ]
    }
   ],
   "source": [
    "#x_gen = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "x_gen_text = \"Name\"\n",
    "x_gen = torch.tensor(encode(x_gen_text), dtype=torch.long, device=device)\n",
    "x_gen = x_gen.view(1, x_gen.shape[0])\n",
    "response = naruto.generate(x_gen, 100)\n",
    "result = decode(response[0].tolist())\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrovision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
